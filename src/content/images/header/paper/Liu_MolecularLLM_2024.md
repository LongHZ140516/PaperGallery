---
title: "A quantitative analysis of knowledge-learning preferences in large language models in molecular science"
authors: ["Pengfei Liu","Jun Tao","Zhixiang Ren"]
year: "2024"
conference: "Nature Machine Intelligence"
tags: ["Nature","LLM"]
license: "CC BY"
image: "Liu_MolecularLLM_2024.webp"
paper: "https://arxiv.org/abs/2402.04119"
code: "https://github.com/AI-HPC-Research-Team/SLM4Mol"
project: ""
bibtex: "@article{liu2025quantitative,
  title={A quantitative analysis of knowledge-learning preferences in large language models in molecular science},
  author={Liu, Pengfei and Tao, Jun and Ren, Zhixiang},
  journal={Nature Machine Intelligence},
  volume={7},
  number={2},
  pages={315--327},
  year={2025},
  publisher={Nature Publishing Group UK London}
}"
---

Deep learning has significantly advanced molecular modelling and design, enabling an efficient understanding and discovery of novel molecules. In particular, large language models introduce a fresh research paradigm to tackle scientific problems from a natural language processing perspective. Large language models significantly enhance our understanding and generation of molecules, often surpassing existing methods with their capabilities to decode and synthesize complex molecular patterns. However, two key issues remain: how to quantify the match between model and data modalities and how to identify the knowledge-learning preferences of models. To address these challenges, we propose a multimodal benchmark, named ChEBI-20-MM, and perform 1,263 experiments to assess the modelâ€™s compatibility with data modalities and knowledge acquisition. Through the modal transition probability matrix, we provide insights into the most suitable modalities for tasks. Furthermore, we introduce a statistically interpretable approach to discover context-specific knowledge mapping by localized feature filtering. Our analysis offers an exploration of the learning mechanism and paves the way for advancing large language models in molecular science.
