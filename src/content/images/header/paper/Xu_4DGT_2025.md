---
title: "4DGT: Learning a 4D Gaussian Transformer Using Real-World Monocular Videos"
authors: ["Zhen Xu", "Zhengqin Li", "Zhao Dong", "Xiaowei Zhou", "Richard Newcombe", "Zhaoyang Lv"]
year: "2025"
conference: "NeurIPS"
license: "CC BY-NC-SA"
tags: ["4DGS", "Monocular", "Video"]
image: "Xu_4DGT_2025.jpg"
paper: "https://arxiv.org/pdf/2506.08015"
code: "https://github.com/facebookresearch/4dgt"
project: "https://4dgt.github.io/"
bibtex: "@article{xu20254dgt,
  title={4dgt: Learning a 4d gaussian transformer using real-world monocular videos},
  author={Xu, Zhen and Li, Zhengqin and Dong, Zhao and Zhou, Xiaowei and Newcombe, Richard and Lv, Zhaoyang},
  journal={arXiv preprint arXiv:2506.08015},
  year={2025}
}"
---

We propose 4DGT, a 4D Gaussian-based Transformer model for dynamic scene reconstruction, trained entirely on real-world monocular posed videos. Using 4D Gaussian as an inductive bias, 4DGT unifies static and dynamic components, enabling the modeling of complex, time-varying environments with varying object lifespans. We proposed a novel density control strategy in training, which enables our 4DGT to handle longer space-time input and remain efficient rendering at runtime. Our model processes 64 consecutive posed frames in a rolling-window fashion, predicting consistent 4D Gaussians in the scene. Unlike optimization-based methods, 4DGT performs purely feed-forward inference, reducing reconstruction time from hours to seconds and scaling effectively to long video sequences. Trained only on large-scale monocular posed video datasets, 4DGT can outperform prior Gaussian-based networks significantly in real-world videos and achieve on-par accuracy with optimization-based methods on cross-domain videos.
