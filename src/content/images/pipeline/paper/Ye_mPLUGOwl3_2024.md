---
title: "mPLUG-Owl3: Towards Long Image-Sequence Understanding in Multi-Modal Large Language Models"
authors: ["Jiabo Ye", "Haiyang Xu", "Haowei Liu", "Anwen Hu", "Ming Yan", "Qi Qian", "Ji Zhang", "Fei Huang", "Jingren Zhou"]
year: "2024"
conference: "arXiv"
license: "CC BY"
tags: ["LLM","Image Understanding"]
image: "Ye_mPLUGOwl3_2024.webp"
paper: "https://www.arxiv.org/pdf/2408.04840"
code: "https://github.com/X-PLUG/mPLUG-Owl"
project: ""
bibtex: "@article{ye2024mplug,
  title={mplug-owl3: Towards long image-sequence understanding in multi-modal large language models},
  author={Ye, Jiabo and Xu, Haiyang and Liu, Haowei and Hu, Anwen and Yan, Ming and Qian, Qi and Zhang, Ji and Huang, Fei and Zhou, Jingren},
  journal={arXiv preprint arXiv:2408.04840},
  year={2024}
}"
---

Multi-modal Large Language Models (MLLMs) have demonstrated remarkable capabilities in executing instructions for a variety of single-image tasks. Despite this progress, significant challenges remain in modeling long image sequences. In this work, we introduce the versatile multi-modal large language model, mPLUG-Owl3, which enhances the capability for long image-sequence understanding in scenarios that incorporate retrieved image-text knowledge, interleaved image-text, and lengthy videos. Specifically, we propose novel hyper attention blocks to efficiently integrate vision and language into a common language-guided semantic space, thereby facilitating the processing of extended multi-image scenarios. Extensive experimental results suggest that mPLUG-Owl3 achieves state-of-the-art performance among models with a similar size on single-image, multi-image, and video benchmarks. Moreover, we propose a challenging long visual sequence evaluation named Distractor Resistance to assess the ability of models to maintain focus amidst distractions. Finally, with the proposed architecture, mPLUG-Owl3 demonstrates outstanding performance on ultra-long visual sequence inputs. We hope that mPLUG-Owl3 can contribute to the development of more efficient and powerful multimodal large language models.
