---
title: "AerialMind: Towards Referring Multi-Object Tracking in UAV Scenarios"
authors: ["Chenglizhao Chen", "Shaofeng Liang", "Runwei Guan", "Xiaolou Sun", "Haocheng Zhao", "Haiyun Jiang", "Tao Huang", "Henghui Ding", "Qing-Long Han"]
year: "2026"
conference: "AAAI"
license: "CC BY"
tags: ["UAV", "Tracking"]
image: "Chen_AerialMind_2026_2.jpg"
paper: "https://arxiv.org/abs/2511.21053"
code: ""
project: ""
bibtex: "@article{chen2025aerialmind,
  title={AerialMind: Towards Referring Multi-Object Tracking in UAV Scenarios},
  author={Chen, Chenglizhao and Liang, Shaofeng and Guan, Runwei and Sun, Xiaolou and Zhao, Haocheng and Jiang, Haiyun and Huang, Tao and Ding, Henghui and Han, Qing-Long},
  journal={arXiv preprint arXiv:2511.21053},
  year={2025}
}"
---

Referring Multi-Object Tracking (RMOT) aims to achieve precise object detection and tracking through natural language instructions, representing a fundamental capability for intelligent robotic systems. However, current RMOT research remains mostly confined to ground-level scenarios, which constrains their ability to capture broad-scale scene contexts and perform comprehensive tracking and path planning. In contrast, Unmanned Aerial Vehicles (UAVs) leverage their expansive aerial perspectives and superior maneuverability to enable wide-area surveillance. Moreover, UAVs have emerged as critical platforms for Embodied Intelligence, which has given rise to an unprecedented demand for intelligent aerial systems capable of natural language interaction. To this end, we introduce AerialMind, the first large-scale RMOT benchmark in UAV scenarios, which aims to bridge this research gap. To facilitate its construction, we develop an innovative semi-automated collaborative agent-based labeling assistant (COALA) framework that significantly reduces labor costs while maintaining annotation quality. Furthermore, we propose HawkEyeTrack (HETrack), a novel method that collaboratively enhances vision-language representation learning and improves the perception of UAV scenarios. Comprehensive experiments validated the challenging nature of our dataset and the effectiveness of our method.
